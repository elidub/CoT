
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=0, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=False, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:04<00:04,  2.28s/it] 75%|███████▌  | 3/4 [00:09<00:03,  3.20s/it]100%|██████████| 4/4 [00:13<00:00,  3.52s/it]100%|██████████| 4/4 [00:13<00:00,  3.28s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 18.4127, 'eval_samples_per_second': 6.572, 'eval_steps_per_second': 0.217}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=0, rebuild_cache=True, shuffle_cots=False, step_by_step=True, no_explanation=False, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:04<00:04,  2.33s/it] 75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]100%|██████████| 4/4 [00:13<00:00,  3.59s/it]100%|██████████| 4/4 [00:13<00:00,  3.35s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 19.1431, 'eval_samples_per_second': 6.321, 'eval_steps_per_second': 0.209}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=1, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:06<00:06,  3.22s/it] 75%|███████▌  | 3/4 [00:12<00:04,  4.54s/it]100%|██████████| 4/4 [00:18<00:00,  4.86s/it]100%|██████████| 4/4 [00:18<00:00,  4.56s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.03305785123966942, 'eval_runtime': 25.6111, 'eval_samples_per_second': 4.725, 'eval_steps_per_second': 0.156}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=2, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:08<00:08,  4.29s/it] 75%|███████▌  | 3/4 [00:17<00:06,  6.05s/it]100%|██████████| 4/4 [00:24<00:00,  6.39s/it]100%|██████████| 4/4 [00:24<00:00,  6.02s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.06611570247933884, 'eval_runtime': 33.771, 'eval_samples_per_second': 3.583, 'eval_steps_per_second': 0.118}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=3, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:11<00:11,  5.64s/it] 75%|███████▌  | 3/4 [00:22<00:07,  7.96s/it]100%|██████████| 4/4 [00:31<00:00,  8.33s/it]100%|██████████| 4/4 [00:31<00:00,  7.86s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.1487603305785124, 'eval_runtime': 44.4893, 'eval_samples_per_second': 2.72, 'eval_steps_per_second': 0.09}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=4, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:14<00:14,  7.11s/it] 75%|███████▌  | 3/4 [00:28<00:10, 10.04s/it]100%|██████████| 4/4 [00:39<00:00, 10.48s/it]100%|██████████| 4/4 [00:39<00:00,  9.90s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.1487603305785124, 'eval_runtime': 56.2638, 'eval_samples_per_second': 2.151, 'eval_steps_per_second': 0.071}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=512, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='arithmetic', dataset_is_bigbench=True, arithmetic_task_name='3_digit_division', bigbench_explanations_dataset='arithmetic_3_digit_division', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=5, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
Found cached dataset bigbench (/project/gpuuva021/shared/cot/hf_cache/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:17<00:17,  8.68s/it] 75%|███████▌  | 3/4 [00:34<00:12, 12.26s/it]100%|██████████| 4/4 [00:48<00:00, 12.76s/it]100%|██████████| 4/4 [00:48<00:00, 12.06s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.12396694214876033, 'eval_runtime': 67.9784, 'eval_samples_per_second': 1.78, 'eval_steps_per_second': 0.059}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=0, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=False, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 550.25it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 592.71it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
{'eval_loss': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 56.5637, 'eval_samples_per_second': 3.129, 'eval_steps_per_second': 0.106}
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:09<00:19,  4.82s/it] 50%|█████     | 3/6 [00:18<00:19,  6.48s/it] 67%|██████▋   | 4/6 [00:28<00:15,  7.67s/it] 83%|████████▎ | 5/6 [00:40<00:09,  9.31s/it]100%|██████████| 6/6 [00:45<00:00,  7.87s/it]100%|██████████| 6/6 [00:45<00:00,  7.58s/it]

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=0, rebuild_cache=True, shuffle_cots=False, step_by_step=True, no_explanation=False, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 747.05it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 492.93it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:10<00:20,  5.02s/it] 50%|█████     | 3/6 [00:19<00:20,  6.78s/it] 67%|██████▋   | 4/6 [00:29<00:16,  8.01s/it] 83%|████████▎ | 5/6 [00:42<00:09,  9.85s/it]100%|██████████| 6/6 [00:47<00:00,  8.29s/it]100%|██████████| 6/6 [00:47<00:00,  7.97s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 59.4551, 'eval_samples_per_second': 2.977, 'eval_steps_per_second': 0.101}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=1, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 741.57it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 571.74it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:12<00:25,  6.41s/it] 50%|█████     | 3/6 [00:24<00:26,  8.71s/it] 67%|██████▋   | 4/6 [00:37<00:20, 10.26s/it] 83%|████████▎ | 5/6 [00:54<00:12, 12.43s/it]100%|██████████| 6/6 [01:00<00:00, 10.41s/it]100%|██████████| 6/6 [01:00<00:00, 10.08s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.480225988700565, 'eval_runtime': 75.1064, 'eval_samples_per_second': 2.357, 'eval_steps_per_second': 0.08}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=2, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 738.63it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 637.63it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:16<00:33,  8.40s/it] 50%|█████     | 3/6 [00:32<00:34, 11.49s/it] 67%|██████▋   | 4/6 [00:49<00:26, 13.48s/it] 83%|████████▎ | 5/6 [01:10<00:16, 16.13s/it]100%|██████████| 6/6 [01:18<00:00, 13.50s/it]100%|██████████| 6/6 [01:18<00:00, 13.12s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.4858757062146893, 'eval_runtime': 98.0071, 'eval_samples_per_second': 1.806, 'eval_steps_per_second': 0.061}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=3, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 562.39it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 609.81it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:23<00:47, 11.85s/it] 50%|█████     | 3/6 [00:46<00:49, 16.36s/it] 67%|██████▋   | 4/6 [01:10<00:38, 19.12s/it] 83%|████████▎ | 5/6 [01:39<00:22, 22.80s/it]100%|██████████| 6/6 [01:51<00:00, 19.09s/it]100%|██████████| 6/6 [01:51<00:00, 18.58s/it]
{'eval_loss': 0.0, 'eval_accuracy': 0.327683615819209, 'eval_runtime': 139.1224, 'eval_samples_per_second': 1.272, 'eval_steps_per_second': 0.043}

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=4, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 570.58it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 493.51it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
{'eval_loss': 0.0, 'eval_accuracy': 0.3107344632768362, 'eval_runtime': 183.9585, 'eval_samples_per_second': 0.962, 'eval_steps_per_second': 0.033}
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:31<01:02, 15.65s/it] 50%|█████     | 3/6 [01:01<01:05, 21.70s/it] 67%|██████▋   | 4/6 [01:32<00:50, 25.31s/it] 83%|████████▎ | 5/6 [02:11<00:29, 29.90s/it]100%|██████████| 6/6 [02:27<00:00, 25.19s/it]100%|██████████| 6/6 [02:27<00:00, 24.51s/it]

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /home/lcur1112/.conda/envs/cot/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
Namespace(train=False, wandb_run=None, model_id='bigscience/bloom-1b1', hf_cache_dir='/project/gpuuva021/shared/cot/hf_cache', debug=False, model_max_length=1015, preprocessed_dir='/project/gpuuva021/shared/cot/data/preprocessed', dataset_name='presuppositions_as_nli', dataset_is_bigbench=True, arithmetic_task_name=None, bigbench_explanations_dataset='presuppositions_as_nli', bigbench_explanations_type='handtuned', bigbench_explanations_path='data/bigbench-explanations/', n_shot=5, rebuild_cache=True, shuffle_cots=False, step_by_step=False, no_explanation=True, qae=False, lr=0.001, max_epochs=10, batch_size=32, seed=666, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none')
/project/gpuuva021/shared/cot/hf_cache
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 541.79it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
Found cached dataset json (/home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 613.43it/s]
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-3469f907dcb2dada.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5767d5db5f3e8e3d.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1ed7528e33f9afaf.arrow
Loading cached processed dataset at /home/lcur1112/.cache/huggingface/datasets/json/default-ba76fde3db5b84f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-460a42fcdacbf8e9.arrow
No finetuning or evaluation run specified, this is an ablation run, not using LoRA!
Using AblationTrainer
Evaluating!
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Traceback (most recent call last):
  File "/home/lcur1112/CoT/cot/run.py", line 116, in <module>
    main(args)
  File "/home/lcur1112/CoT/cot/run.py", line 109, in main
    run_model(model, tokenizer, tokenized_datasets, args)
  File "/home/lcur1112/CoT/cot/../cot/learner.py", line 394, in run_model
    eval = trainer.evaluate()
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 3029, in evaluate
    output = eval_loop(
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 3210, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/trainer.py", line 3483, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/lcur1112/CoT/cot/../cot/trainer.py", line 29, in compute_loss
    outputs = model.generate(input_ids = input_ids, max_new_tokens = 32)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 1515, in generate
    return self.greedy_search(
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 2332, in greedy_search
    outputs = self(
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py", line 926, in forward
    lm_logits = self.lm_head(hidden_states)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.44 GiB (GPU 0; 23.65 GiB total capacity; 9.85 GiB already allocated; 4.58 GiB free; 18.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: r36n2: task 0: Exited with exit code 1
