#!/bin/bash

#SBATCH --gres=gpu:1
#SBATCH --job-name=NLITraining
#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=12:00:00
#SBATCH --mem=32000M
#SBATCH --output=jobs/slurm_output/train-nli_%A.out

module purge
module load 2021
module load Anaconda3/2021.05

export WANDB_API_KEY=6b6f51f692ebb5799c9e3ee77ff79b3622fc9370

cd $HOME/CoT/
source activate cot

srun python -u cot/run.py --model_id bigscience/bloom-3b \
    --batch_size 4 --n_shot 4 --max_epochs 50  --lr 0.0001 \
    --reward_succesful_explanations --allow_answer_at_eos \
    --dataset_name presuppositions_as_nli --bigbench_explanations_dataset presuppositions_as_nli --model_max_length 1015 --train
    