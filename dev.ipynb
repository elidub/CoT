{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: Loading binary /home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1112/.conda/envs/cot/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from cot.run import debug_parse_option\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from cot.dataset import CoTDataset\n",
    "from cot.learner import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = debug_parse_option(notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766efa1629e44aa49880648b65b12d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875b21911e74406481d73d8b9504940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_id, cache_dir=args.hf_cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_id, cache_dir=args.hf_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bigbench (/home/lcur1112/CoT/datadump/hf/tasksource___bigbench/arithmetic/1.0.0/c5da5ac497141c7435da10444495b8577405d4ed01e524265b144a7063718c0c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a subset of the dataset\n",
      "\n",
      "Q: What is 842 divided by 1?\n",
      "  choice: 456\n",
      "  choice: 842\n",
      "  choice: house\n",
      "  choice: 14513\n",
      "  choice: 1\n",
      "  choice: banana\n",
      "  choice: 820\n",
      "1: 842\n",
      "Explanation: Dividing any number by 1 does not change it, so 842 / 1 = 842.\n",
      "\n",
      "Q: What is 355 divided by 71?\n",
      "  choice: 5\n",
      "  choice: banana\n",
      "  choice: 807\n",
      "  choice: 7\n",
      "  choice: house\n",
      "  choice: 6\n",
      "1: 5\n",
      "Explanation: 355 is close to 400, and 71 is close to 80, and 400 / 80 = 5, so we can guess that the answer must be 5.\n",
      "\n",
      "Q: What is 705 divided by 5?\n",
      "  choice: 445\n",
      "  choice: 113\n",
      "  choice: 141\n",
      "  choice: banana\n",
      "  choice: house\n",
      "  choice: 73442\n",
      "  choice: 2\n",
      "1: 141\n",
      "Explanation: 705 / 5 = (700 / 5) + (5 / 5) = 140 + 1 = 141.\n",
      "\n",
      "Q: What is 680 divided by 68?\n",
      "  choice: 59\n",
      "  choice: 4009\n",
      "  choice: banana\n",
      "  choice: 4\n",
      "  choice: house\n",
      "  choice: 86\n",
      "  choice: 10\n",
      "1: 10\n",
      "Explanation: 68 * 10 = 680, so 680 / 68 = 10.\n",
      "\n",
      "Q: What is 980 divided by 5?\n",
      "  choice: house\n",
      "  choice: 89481\n",
      "  choice: banana\n",
      "  choice: 823\n",
      "  choice: 225\n",
      "  choice: 196\n",
      "  choice: 8\n",
      "1: 196\n",
      "Explanation: We can rewrite as follows: 980 / 5 = (1000 - 20) / 5 = (1000 / 5) - (20 / 5) = 200 - 4 = 196.\n",
      "\n",
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m splits \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits:\n\u001b[0;32m----> 4\u001b[0m     tokenized_datasets[split] \u001b[39m=\u001b[39m CoTDataset(args, tokenizer, split)\n",
      "File \u001b[0;32m~/CoT/cot/dataset.py:155\u001b[0m, in \u001b[0;36mCoTDataset.__init__\u001b[0;34m(self, config, tokenizer, split)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39m# print(\"AAAAAAAAAAAAAAAAAAAAAAa\")\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39m# print(tokenized_dataset['inputs_untokenized'][0])\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[39m# print(tokenized_dataset['labels_untokenized'][0])\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[39m# Tokenize cot's\u001b[39;00m\n\u001b[1;32m    154\u001b[0m handtuned_file_path \u001b[39m=\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbigbench_explanations_path) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbigbench_explanations_type \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbigbench_explanations_dataset \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m questions, answers, explanations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_handtuned(handtuned_file_path)\n\u001b[1;32m    156\u001b[0m \u001b[39m# tokenized_explanations = [self.tokenizer(questions[i] + answers[i] + explanations[i]) for i in range(len(questions))]\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m#TODO! CHECK IF CONFIG.QAE IS TRUE WHEN APPENDING STEP BY STEP\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[39m# In zero-shot CoT setting we have the option to append \"Let's think this step by step\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mstep_by_step \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_shot \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/CoT/cot/dataset.py:235\u001b[0m, in \u001b[0;36mCoTDataset.parse_handtuned\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    233\u001b[0m substrings \u001b[39m=\u001b[39m full_string\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_prefix)[\u001b[39m1\u001b[39m:]  \u001b[39m# Split the string and discard the first element\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mfor\u001b[39;00m substring \u001b[39min\u001b[39;00m substrings:\n\u001b[0;32m--> 235\u001b[0m     q, a_explanation \u001b[39m=\u001b[39m substring\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswer_prefix, \u001b[39m1\u001b[39m)  \u001b[39m# Split each substring into question and remaining part\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     a, explanation \u001b[39m=\u001b[39m a_explanation\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplanation_prefix, \u001b[39m1\u001b[39m)  \u001b[39m# Split the remaining part into answer and explanation\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     q_list\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_prefix \u001b[39m+\u001b[39m q\u001b[39m.\u001b[39mstrip() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)  \u001b[39m# Add \"Q: \" prefix and strip any leading/trailing whitespaces\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = {}\n",
    "splits = [\"train\", \"validation\"]\n",
    "for split in splits:\n",
    "    tokenized_datasets[split] = CoTDataset(args, tokenizer, split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
