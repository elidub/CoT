t5-small:
  model: T5ForConditionalGeneration
  tokenizer: T5Tokenizer
  model_kwargs: 
    device_map: auto
    load_in_8bit: True
t5-base:
  model: T5ForConditionalGeneration
  tokenizer: T5Tokenizer
  model_kwargs: 
    device_map: auto
    load_in_8bit: True
t5-large:
  model: AutoModelForSeq2SeqLM
  tokenizer: AutoTokenizer
  model_kwargs: 
    device_map: auto
    load_in_8bit: True
t5-3b:
  model: AutoModelForSeq2SeqLM
  tokenizer: AutoTokenizer
  model_kwargs: 
    device_map: auto
    load_in_8bit: True
# google/flan-t5-small:
#   model: T5ForConditionalGeneration
#   tokenizer: T5Tokenizer
#   model_kwargs: 
#     device_map: auto
#     load_in_8bit: True
# google/flan-t5-base:
#   model: T5ForConditionalGeneration
#   tokenizer: T5Tokenizer
#   model_kwargs: 
#     device_map: auto
#     load_in_8bit: True
# google/flan-t5-large:
#   model: AutoModelForSeq2SeqLM
#   tokenizer: AutoTokenizer
#   model_kwargs: 
#     device_map: auto
#     load_in_8bit: True
# bigscience/T0_3B:
#   model: AutoModelForSeq2SeqLM
#   tokenizer: AutoTokenizer
# bigscience/T0pp:
#   model: AutoModelForSeq2SeqLM
#   tokenizer: AutoTokenizer
# bigscience/bloom:
#   model: AutoModelForCausalLM
#   tokenizer: AutoTokenizer
#   model_kwargs: 
#     device_map: auto
#     torch_dtype: auto